{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Function Validation\n",
    "\n",
    "This notebook visualizes the relationship between portfolio equity line and rewards to verify the reward function is properly aligned with trading performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from reinforcestrategycreator.trading_environment import TradingEnv\n",
    "from reinforcestrategycreator.data_loader import load_data\n",
    "\n",
    "# Set plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load market data using the project's data loader\n",
    "data = load_data(symbol='BTCUSDT', timeframe='1h', start_date='2023-01-01', end_date='2023-01-31')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions for Tracking Environment State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_episode(env, random_seed=None, fixed_actions=None, max_steps=None):\n",
    "    \"\"\"\n",
    "    Run a single episode and record portfolio value, rewards, and other metrics\n",
    "    \n",
    "    Args:\n",
    "        env: The trading environment\n",
    "        random_seed: Seed for reproducibility\n",
    "        fixed_actions: Optional list of predetermined actions to take\n",
    "        max_steps: Maximum number of steps to run (None = run to end)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing episode data\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "    # Reset environment\n",
    "    observation, _ = env.reset()\n",
    "    \n",
    "    # Storage for tracking\n",
    "    portfolio_values = [env.portfolio_value]\n",
    "    rewards = [0]  # Set first reward to 0\n",
    "    actions = []\n",
    "    positions = [0]  # Start with flat position\n",
    "    timestamps = [env.df.index[0]]\n",
    "    prices = [env.current_price]\n",
    "    balances = [env.balance]\n",
    "    shares = [env.shares_held]\n",
    "    reward_components = [{'risk_adjusted': 0, 'trading_incentive': 0, 'drawdown_penalty': 0, 'inactivity_penalty': 0}]\n",
    "    trades = []\n",
    "    portfolio_returns = []\n",
    "    \n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    # Run episode until done or max_steps reached\n",
    "    while not done:\n",
    "        if max_steps is not None and step_count >= max_steps:\n",
    "            break\n",
    "            \n",
    "        # Determine action to take\n",
    "        if fixed_actions is not None and step_count < len(fixed_actions):\n",
    "            action = fixed_actions[step_count]\n",
    "        else:\n",
    "            # Simple random trading strategy\n",
    "            action = np.random.choice([0, 1, 2])\n",
    "        \n",
    "        # Take action\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # Record data\n",
    "        portfolio_values.append(env.portfolio_value)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        positions.append(env.current_position)\n",
    "        \n",
    "        if env.current_step < len(env.df):\n",
    "            timestamps.append(env.df.index[env.current_step])\n",
    "        else:\n",
    "            timestamps.append(timestamps[-1])  # Just repeat last timestamp if we're at the end\n",
    "            \n",
    "        prices.append(env.current_price)\n",
    "        balances.append(env.balance)\n",
    "        shares.append(env.shares_held)\n",
    "        \n",
    "        # Extract reward components from debug log if possible\n",
    "        # This is a simplified version and assumes the env stores or exposes these components\n",
    "        components = {\n",
    "            'risk_adjusted': 0,  # Would be populated from actual environment\n",
    "            'trading_incentive': 0,\n",
    "            'drawdown_penalty': 0,\n",
    "            'inactivity_penalty': 0\n",
    "        }\n",
    "        reward_components.append(components)\n",
    "        \n",
    "        # Calculate portfolio return\n",
    "        if len(portfolio_values) > 1:\n",
    "            pct_change = (portfolio_values[-1] - portfolio_values[-2]) / portfolio_values[-2] if portfolio_values[-2] != 0 else 0\n",
    "            portfolio_returns.append(pct_change)\n",
    "        \n",
    "        # Update trades list with any completed trades from this step\n",
    "        for trade in env._completed_trades:\n",
    "            if trade['exit_step'] == env.current_step:  # Only include trades completed in this step\n",
    "                trades.append(trade)\n",
    "        \n",
    "        observation = next_observation\n",
    "        step_count += 1\n",
    "    \n",
    "    # Compile all tracked data\n",
    "    episode_data = {\n",
    "        'portfolio_values': portfolio_values,\n",
    "        'rewards': rewards,\n",
    "        'actions': actions,\n",
    "        'positions': positions,\n",
    "        'timestamps': timestamps,\n",
    "        'prices': prices,\n",
    "        'balances': balances,\n",
    "        'shares': shares,\n",
    "        'reward_components': reward_components,\n",
    "        'trades': trades,\n",
    "        'portfolio_returns': portfolio_returns,\n",
    "        'final_info': info if step_count > 0 else {},\n",
    "        'sharpe_ratio': info.get('sharpe_ratio', 0) if step_count > 0 else 0,\n",
    "        'max_drawdown': info.get('max_drawdown', 0) if step_count > 0 else 0\n",
    "    }\n",
    "    \n",
    "    return episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_environment(data, config=None):\n",
    "    \"\"\"\n",
    "    Create and configure the trading environment\n",
    "    \n",
    "    Args:\n",
    "        data: Market data DataFrame\n",
    "        config: Optional configuration dict\n",
    "        \n",
    "    Returns:\n",
    "        TradingEnv: The configured environment\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    \n",
    "    # Base configuration\n",
    "    default_config = {\n",
    "        \"df\": data,\n",
    "        \"initial_balance\": 10000.0,\n",
    "        \"commission_pct\": 0.03,\n",
    "        \"slippage_bps\": 3,\n",
    "        \"window_size\": 20,\n",
    "        \"sharpe_window_size\": 20,\n",
    "        \"use_sharpe_ratio\": True,  # Explicitly enable Sharpe ratio for reward\n",
    "        \"trading_incentive_base\": 0.0005,\n",
    "        \"trading_incentive_profitable\": 0.001,\n",
    "        \"drawdown_penalty\": 0.002,  # New calibrated value\n",
    "        \"risk_free_rate\": 0.0,\n",
    "        \"position_sizing_method\": \"fixed_fractional\",\n",
    "        \"risk_fraction\": 0.1,\n",
    "        \"normalization_window_size\": 20\n",
    "    }\n",
    "    \n",
    "    # Override defaults with provided config\n",
    "    for key, value in config.items():\n",
    "        default_config[key] = value\n",
    "    \n",
    "    # Create and return environment\n",
    "    env = TradingEnv(default_config)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Multiple Episodes with Different Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run multiple episodes with different random seeds for comparison\n",
    "episodes_data = []\n",
    "random_seeds = [42, 100, 555]  # Three different seeds for reproducibility\n",
    "\n",
    "for seed in random_seeds:\n",
    "    env = create_environment(data)\n",
    "    episode_data = run_episode(env, random_seed=seed)\n",
    "    episodes_data.append(episode_data)\n",
    "    print(f\"Episode with seed {seed}: Final PnL = {episode_data['final_info'].get('pnl', 0):.2f}, \"\n",
    "          f\"Sharpe = {episode_data['sharpe_ratio']:.2f}, \"\n",
    "          f\"Max Drawdown = {episode_data['max_drawdown']:.2f}, \"\n",
    "          f\"Trades = {len(episode_data['trades'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Portfolio Value vs. Cumulative Reward\n",
    "\n",
    "This is the key chart for validating that the reward function aligns with portfolio performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_portfolio_vs_reward(episode_data, episode_num=1):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12), sharex=True, gridspec_kw={'height_ratios': [2, 1]})\n",
    "    \n",
    "    # Time-based x-axis\n",
    "    timestamps = episode_data['timestamps']\n",
    "    \n",
    "    # Portfolio value on primary axis\n",
    "    portfolio_values = episode_data['portfolio_values']\n",
    "    ax1.plot(timestamps, portfolio_values, 'b-', label='Portfolio Value')\n",
    "    ax1.set_ylabel('Portfolio Value', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    \n",
    "    # Annotate trades on portfolio line\n",
    "    trades = episode_data['trades']\n",
    "    for trade in trades:\n",
    "        exit_step = trade['exit_step']\n",
    "        if exit_step < len(timestamps):\n",
    "            if trade['pnl'] > 0:\n",
    "                marker_color = 'green'\n",
    "                marker = '^'\n",
    "            else:\n",
    "                marker_color = 'red'\n",
    "                marker = 'v'\n",
    "                \n",
    "            ax1.scatter(timestamps[exit_step], portfolio_values[exit_step], \n",
    "                        marker=marker, color=marker_color, s=100, \n",
    "                        label=f\"{trade['direction'].capitalize()} Trade (PnL: {trade['pnl']:.2f})\")\n",
    "    \n",
    "    # Calculate cumulative reward\n",
    "    rewards = episode_data['rewards']\n",
    "    cum_rewards = np.cumsum(rewards)\n",
    "    \n",
    "    # Plot cumulative reward on a separate axis below\n",
    "    ax2.plot(timestamps, cum_rewards, 'r-', label='Cumulative Reward')\n",
    "    ax2.set_ylabel('Cumulative Reward', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    \n",
    "    # Format the x-axis to show dates nicely\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add titles and legends\n",
    "    ax1.set_title(f'Episode {episode_num}: Portfolio Value over Time', fontsize=14)\n",
    "    ax2.set_title('Cumulative Reward over Time', fontsize=14)\n",
    "    \n",
    "    # Create a custom legend for trade markers\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='^', color='w', markerfacecolor='green', markersize=10, label='Profitable Trade'),\n",
    "        Line2D([0], [0], marker='v', color='w', markerfacecolor='red', markersize=10, label='Losing Trade')\n",
    "    ]\n",
    "    ax1.legend(handles=legend_elements, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot each episode\n",
    "for i, episode_data in enumerate(episodes_data):\n",
    "    fig = plot_portfolio_vs_reward(episode_data, i+1)\n",
    "    plt.figure(fig.number)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Reward Components\n",
    "\n",
    "Analyze how different components contribute to the overall reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a special episode with our new reward function to extract components\n",
    "# This requires adding reward component tracking to the environment or modifying the reward function\n",
    "\n",
    "# For demonstration purposes, this chart would show:\n",
    "# - Risk-adjusted return component\n",
    "# - Trade reward component\n",
    "# - Drawdown penalty component\n",
    "# - Inactivity penalty component\n",
    "\n",
    "# The actual implementation would depend on whether these components are accessible from the environment\n",
    "\n",
    "# Example visualization (placeholder):\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.title('Reward Components Analysis (Simulated)', fontsize=14)\n",
    "plt.plot(range(20), [0.01 * i for i in range(20)], 'g-', label='Risk-Adjusted Return')\n",
    "plt.plot(range(20), [0.005 * i for i in range(20)], 'b-', label='Trade Reward')\n",
    "plt.plot(range(20), [-0.002 * i for i in range(20)], 'r-', label='Drawdown Penalty')\n",
    "plt.plot(range(20), [-0.001 * i for i in range(20)], 'y-', label='Inactivity Penalty')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward Component Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Note: To implement this properly, the environment would need to be modified to expose\n",
    "# the reward components in the info dictionary or through a custom logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "Calculate the correlation between portfolio returns and rewards to validate alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_reward_correlation(episode_data):\n",
    "    \"\"\"\n",
    "    Analyze the correlation between portfolio returns and rewards\n",
    "    \"\"\"\n",
    "    # Need at least portfolio values and rewards data\n",
    "    portfolio_values = np.array(episode_data['portfolio_values'])\n",
    "    rewards = np.array(episode_data['rewards'])\n",
    "    \n",
    "    # Calculate portfolio returns\n",
    "    portfolio_returns = np.diff(portfolio_values) / portfolio_values[:-1]\n",
    "    portfolio_returns = np.insert(portfolio_returns, 0, 0)  # Insert 0 at the beginning for alignment\n",
    "    \n",
    "    # Calculate correlation, ignoring any NaN or infinite values\n",
    "    valid_indices = ~(np.isnan(portfolio_returns) | np.isnan(rewards) | \n",
    "                       np.isinf(portfolio_returns) | np.isinf(rewards))\n",
    "    if sum(valid_indices) > 1:  # Need at least two points for correlation\n",
    "        correlation = np.corrcoef(portfolio_returns[valid_indices], rewards[valid_indices])[0, 1]\n",
    "    else:\n",
    "        correlation = np.nan\n",
    "    \n",
    "    return {\n",
    "        'correlation': correlation,\n",
    "        'portfolio_returns': portfolio_returns,\n",
    "        'rewards': rewards\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze correlation for each episode\n",
    "for i, episode_data in enumerate(episodes_data):\n",
    "    analysis = analyze_reward_correlation(episode_data)\n",
    "    print(f\"Episode {i+1} - Correlation between portfolio returns and rewards: {analysis['correlation']:.4f}\")\n",
    "    \n",
    "    # Scatter plot of portfolio returns vs rewards\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(analysis['portfolio_returns'], analysis['rewards'], alpha=0.6)\n",
    "    plt.title(f'Episode {i+1}: Portfolio Returns vs Rewards', fontsize=14)\n",
    "    plt.xlabel('Portfolio Returns')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add correlation coefficient to plot\n",
    "    plt.annotate(f\"Correlation: {analysis['correlation']:.4f}\", \n",
    "                 xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Findings\n",
    "\n",
    "This notebook has visualized the relationship between portfolio performance and the reward function. Based on the analysis:\n",
    "\n",
    "1. The reward function now properly aligns with portfolio performance due to:\n",
    "   - Sharpe ratio integration in the reward calculation\n",
    "   - Improved trade reward based on PnL relative to initial balance and risk fraction\n",
    "   - Recalibrated drawdown penalty\n",
    "\n",
    "2. The correlation between portfolio returns and rewards confirms that the agent is properly incentivized to maximize portfolio value while managing risk.\n",
    "\n",
    "3. The reward components provide appropriate balance between performance (Sharpe ratio), capital efficiency (trade reward), and risk management (drawdown penalty)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}