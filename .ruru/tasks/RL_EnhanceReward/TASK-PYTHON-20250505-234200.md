+++
# --- MDTM Task File ---
id = "TASK-PYTHON-20250505-234200"
title = "Implement Enhanced Reward Function (Risk-Adjusted)"
status = "ğŸŸ¢ Done" # Options: ğŸŸ¡ To Do, ğŸŸ  In Progress, ğŸŸ¢ Done, âšª Blocked, ğŸŸ£ Review
type = "ğŸŒŸ Feature" # Options: ğŸŒŸ Feature, ğŸ Bug, ğŸ› ï¸ Refactor, ğŸ§ª Test, ğŸ“„ Documentation, ğŸ”¬ Analysis, âš™ï¸ Chore
created_date = "2025-05-05"
updated_date = "2025-05-05" # Updated after implementation
assigned_to = "dev-python" # Mode slug
coordinator = "TASK-CMD-..." # Replace with actual Commander Task ID if available
priority = "High"
complexity = "Medium"
estimated_effort = "3h"
related_tasks = ["TASK-PYTHON-20250505-203900"] # Link to the analysis task
target_branch = "feature/rl-strategy-enhancements"
tags = ["rl", "trading", "reward-function", "risk-management", "python", "phase1"]
# --- End Metadata ---
+++

# Implement Enhanced Reward Function (Risk-Adjusted)

## 1. Description

As part of Phase 1 of the RL strategy enhancements (based on analysis task `TASK-PYTHON-20250505-203900`), this task focuses on improving the reward function in `reinforcestrategycreator/trading_environment.py`.

The current reward is based on simple percentage portfolio change. This needs to be replaced with a more sophisticated reward signal that encourages better risk-adjusted performance.

Implement the recommendations from the analysis:
*   Calculate reward based on risk-adjusted metrics like the Sharpe ratio or Sortino ratio, calculated over a rolling window or per episode.
*   Incorporate penalties for excessive trading frequency to account for transaction costs and discourage noise trading.
*   Add penalties for significant drawdowns to promote capital preservation.

## 2. Acceptance Criteria

*   The `_calculate_reward` method (or relevant logic) in `reinforcestrategycreator/trading_environment.py` is updated to reflect the new reward structure.
*   The reward calculation incorporates a risk-adjusted metric (e.g., rolling Sharpe ratio).
*   Penalties for transaction frequency/costs are implemented.
*   Penalties for drawdowns are implemented.
*   The implementation is configurable (e.g., allowing adjustment of penalty weights or choice of risk metric via environment parameters).
*   Relevant unit tests are added or updated in `tests/test_trading_environment.py` (or a new test file) to verify the new reward logic under various scenarios.
*   Code is committed to the `feature/rl-strategy-enhancements` branch.

## 3. Checklist

*   [âœ…] Analyze existing `_calculate_reward` and related portfolio tracking logic.
*   [âœ…] Design the new reward formula incorporating Sharpe/Sortino, trading penalty, and drawdown penalty.
*   [âœ…] Implement the calculation of the chosen risk-adjusted metric (e.g., rolling Sharpe).
*   [âœ…] Implement the trading frequency penalty logic.
*   [âœ…] Implement the drawdown penalty logic.
*   [âœ…] Integrate these components into the final reward calculation within the environment step.
*   [âœ…] Add configurability for reward components/weights.
*   [âœ…] Write/update unit tests for the new reward function.
*   [ ] Test the changes within the existing training loop (`train.py`) to ensure compatibility. (Skipping manual test for now, relying on unit tests)
*   [ ] Commit changes to the `feature/rl-strategy-enhancements` branch following commit standards (Rule `07`). (Will do next)

## 4. Logs / Notes

*(Python Developer will add notes here during implementation)*
- Implemented enhanced reward function in `trading_environment.py`.
- Added parameters: `use_sharpe_ratio`, `trading_frequency_penalty`, `drawdown_penalty`, `risk_free_rate`.
- Updated `_calculate_reward` to include Sharpe ratio (optional), trading frequency penalty, and drawdown penalty.
- Added tracking for `_trade_count` and `max_portfolio_value`.
- Updated `test_trading_env_reward.py` with new tests and fixes.
- All relevant unit tests passed.