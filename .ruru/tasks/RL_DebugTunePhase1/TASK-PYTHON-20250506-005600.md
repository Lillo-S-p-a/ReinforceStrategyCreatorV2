+++
# --- MDTM Task File ---
id = "TASK-PYTHON-20250506-005600"
title = "Investigate & Tune Poor Performance After Phase 1 Enhancements"
status = "üü° To Do" # Options: üü° To Do, üü† In Progress, üü¢ Done, ‚ö™ Blocked, üü£ Review
type = "üêû Bug" # Treating poor performance as a bug/tuning issue
created_date = "2025-05-06"
updated_date = "2025-05-06"
assigned_to = "dev-python" # Mode slug
coordinator = "TASK-CMD-..." # Replace with actual Commander Task ID if available
priority = "Critical" # Needs to be addressed before Phase 2
complexity = "High"
estimated_effort = "4h-8h" # Tuning can be iterative
related_tasks = ["TASK-PYTHON-20250506-003500", "TASK-PYTHON-20250505-234200", "TASK-PYTHON-20250505-234700", "TASK-PYTHON-20250505-235200", "TASK-PYTHON-20250506-000900"] # Link to DB analysis and Phase 1 tasks
target_branch = "feature/rl-strategy-enhancements"
tags = ["rl", "trading", "debugging", "tuning", "hyperparameters", "performance", "python", "phase1"]
# --- End Metadata ---
+++

# Investigate & Tune Poor Performance After Phase 1 Enhancements

## 1. Description

Following the implementation of Phase 1 enhancements (reward, risk, features, normalization) on the `feature/rl-strategy-enhancements` branch, the latest training run (`RUN-SPY-20250505215547-118de704`) exhibited very poor performance (Avg Sharpe: -2.12, Avg Non-HOLD Ops: 61.6).

This task requires investigating the cause of this poor performance and proposing/implementing initial tuning adjustments to the agent hyperparameters and environment configuration.

Focus areas:
1.  Review parameters used in the poorly performing run (agent hyperparameters, reward weights, risk management settings).
2.  Analyze the interaction between the agent's actions, the reward signals received, and the risk management triggers during that run (using DB data if necessary).
3.  Hypothesize the primary reasons for the negative Sharpe ratio (e.g., poorly balanced reward, overly aggressive risk management, unsuitable learning rate).
4.  Propose specific adjustments to hyperparameters (e.g., learning rate, epsilon decay, discount factor), reward component weights, and risk management parameters (SL/TP levels, position sizing fraction).
5.  Optionally, implement these initial adjustments and run a short test training run to observe effects.
6.  Recommend a more systematic approach for hyperparameter optimization if initial adjustments are insufficient (e.g., using Optuna, Ray Tune).

## 2. Acceptance Criteria

*   Analysis of the parameters and potential behavior during run `RUN-SPY-20250505215547-118de704` is documented.
*   Plausible causes for the poor performance are identified.
*   Specific, concrete recommendations for initial tuning adjustments (hyperparameters, reward weights, risk settings) are provided.
*   (Optional) Initial adjustments are implemented in the code on the target branch.
*   (Optional) Results of a short test run with adjusted parameters are documented.
*   A recommendation for further systematic hyperparameter optimization is made if necessary.
*   Findings and recommendations are documented in this task file.

## 3. Checklist

*   [‚úÖ] Retrieve parameters used for run `RUN-SPY-20250505215547-118de704` (from `training_runs` table or logs).
*   [‚úÖ] Analyze agent hyperparameters (learning rate, epsilon, gamma, network size, etc.).
*   [‚úÖ] Analyze environment configuration (reward weights, SL/TP levels, position sizing fraction, normalization window).
*   [ ] (Optional) Query `steps` and `trading_operations` for the run to understand typical rewards, actions, and risk triggers.
*   [‚úÖ] Formulate hypotheses for poor performance.
*   [‚úÖ] Define specific parameter adjustments to test.
*   [‚úÖ] Modify `train.py` or configuration files with adjusted parameters. (Removed reward scaling in env, adjusted params in train.py)
*   [‚úÖ] Execute a short training run (e.g., 10-20 episodes) with new parameters. (Ran RUN-SPY-20250505225840-5ffc28d0 for 10 episodes)
*   [‚úÖ] Analyze results of the test run.
*   [‚úÖ] Document findings, adjustments made/proposed, and recommendations for next steps (including systematic tuning).

## 4. Logs / Notes

*(Python Developer will add analysis, proposed/implemented changes, and recommendations here)*

**Analysis (2025-05-06):**

1.  **Retrieved Parameters:** Successfully fetched parameters for run `RUN-SPY-20250505215547-118de704` from the `training_runs` table in the PostgreSQL database (`trading_db`). Agent hyperparameters (LR=0.001, Gamma=0.95, EpsilonDecay=0.995, etc.) seem standard.
2.  **Environment Configuration (Defaults from `trading_environment.py`):**
    *   **Reward:** `use_sharpe_ratio=True`, `trading_frequency_penalty=0.01`, `drawdown_penalty=0.1`.
    *   **Risk:** `stop_loss_pct=None`, `take_profit_pct=None`, `position_sizing_method="fixed_fractional"`, `risk_fraction=0.1`.
    *   **Normalization:** `normalization_window_size=20`.
3.  **Reward Calculation (`_calculate_reward`):**
    *   The final reward is `risk_adjusted_return - trading_penalty - drawdown_penalty`.
    *   **Key Issue:** The `risk_adjusted_return` (when using Sharpe) is calculated and then **scaled by `0.01`**. This makes the positive reward component extremely small.
4.  **Hypotheses for Poor Performance:**
    *   **Primary:** The `0.01` scaling factor on the Sharpe reward makes the positive signal negligible compared to the penalties, especially the high `drawdown_penalty` (0.1). The agent is likely learning to avoid drawdowns above all else, hindering profitability.
    *   **Secondary:** Aggressive position sizing (`risk_fraction=0.1`) exacerbates the impact of losses and the drawdown penalty. Disabled SL/TP allows losses to run. The trading penalty might discourage potentially viable, more active strategies.

**Proposed Initial Adjustments:**

Focus on rebalancing the reward function and reducing risk per trade:

1.  **Reward Scaling:** **Remove** the `risk_adjusted_return *= 0.01` scaling (line ~692 in `trading_environment.py`). Let the raw Sharpe ratio (or percentage change) contribute directly.
2.  **Drawdown Penalty:** **Reduce** `drawdown_penalty` weight significantly. Start by trying `0.01` or `0.02` (instead of `0.1`).
3.  **Trading Penalty:** Consider slightly reducing `trading_frequency_penalty` to `0.005` to allow more exploration of active strategies, given the high number of operations observed.
4.  **Position Sizing:** **Reduce** `risk_fraction` substantially. Start with `0.02` (2% risk per trade) instead of `0.1`.
5.  **Stop-Loss:** **Enable** a basic Stop-Loss as a safety net. Start with `stop_loss_pct=5.0` (5%).

**Next Steps:**

*   Implement proposed adjustments 1-5 by modifying `trading_environment.py` defaults or passing them during `TradingEnv` initialization in `train.py`.
*   Implemented proposed adjustments 1-5 (Removed reward scaling, adjusted penalties/risk fraction, enabled 5% SL).
*   Executed a 10-episode test run: `RUN-SPY-20250505225840-5ffc28d0`.

**Test Run Analysis (RUN-SPY-20250505225840-5ffc28d0):**

*   **Result:** The agent took **zero** trading actions across all 10 episodes. PnL, Sharpe, Max Drawdown, Win Rate, and Total Reward were all 0.0.
*   **Interpretation:** The agent's policy, learned under the previous flawed reward structure, appears stuck in a local optimum where inaction is perceived as the best strategy to avoid penalties (especially the previously high drawdown penalty). The parameter adjustments were not enough to overcome this learned bias.

**Recommendations:**

1.  **Reset Agent Learning:** The most direct approach is to discard the agent's current learned state (memory/model weights) and start a **fresh training run** using the corrected environment parameters (reward scaling removed, adjusted penalties, reduced risk fraction, SL enabled). This avoids trying to fix the stuck policy.
2.  **Monitor Fresh Run:** Run a longer training session (e.g., 50-100 episodes) with the corrected parameters and monitor metrics closely. Check if the agent starts trading and if performance metrics (Sharpe, PnL) improve compared to the original problematic run.
3.  **Systematic Tuning (If Needed):** If the fresh run still shows poor performance, then proceed with systematic hyperparameter optimization using a library like Optuna or Ray Tune. This would involve tuning not only the environment parameters (penalties, risk) but also agent hyperparameters (learning rate, epsilon decay, network architecture, gamma).

**Current Status:** Initial investigation complete, code adjusted, test run performed. Agent requires retraining from scratch with corrected parameters.