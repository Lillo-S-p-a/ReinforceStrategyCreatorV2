+++
id = "TASK-SDEV-20250513-115900"
title = "Implement Parallel Training using Ray/RLlib"
status = "🟡 To Do"
type = "🌟 Feature"
priority = "🔼 High"
created_date = "2025-05-13"
updated_date = "2025-05-13"
# due_date = ""
# estimated_effort = ""
assigned_to = "util-senior-dev"
# reporter = "roo-commander"
parent_task = "TASK-ARCH-20250513-090800"
depends_on = ["TASK-ARCH-20250513-090800"]
related_docs = [
    ".ruru/tasks/ARCH_ParallelTraining/TASK-ARCH-20250513-090800.md",
    ".ruru/decisions/ADR-001_Parallel_Training_Strategy.md",
    "train.py",
    "reinforcestrategycreator/rl_agent.py",
    "reinforcestrategycreator/trading_environment.py",
    "pyproject.toml"
]
tags = ["parallelization", "ray", "rllib", "implementation", "performance", "training", "senior-dev"]
template_schema_doc = ".ruru/templates/toml-md/01_mdtm_feature.README.md"
# ai_prompt_log = """"""
# review_checklist = []
# reviewed_by = ""
# key_learnings = ""
+++

# Implement Parallel Training using Ray/RLlib

## Description ✍️

*   **What is this feature?** This feature involves refactoring the existing reinforcement learning training pipeline to use Ray and its RLlib library for parallel execution of training environments.
*   **Why is it needed?** To significantly reduce the overall training time, enabling faster development iterations and more extensive hyperparameter tuning. The current sequential training process is a major bottleneck.
*   **Scope:**
    *   Integrate Ray/RLlib into the project.
    *   Adapt the existing `TradingEnv` and `StrategyAgent` (or use RLlib equivalents).
    *   Refactor the main training script (`train.py`).
    *   Implement robust logging and model saving in the parallel setup.
*   **Links:**
    *   Design Task: [TASK-ARCH-20250513-090800](./../ARCH_ParallelTraining/TASK-ARCH-20250513-090800.md)
    *   ADR: [ADR-001_Parallel_Training_Strategy.md](../../decisions/ADR-001_Parallel_Training_Strategy.md)

## Acceptance Criteria ✅

*   - [✅] Ray (`ray[rllib]`) is added as a project dependency in [`pyproject.toml`](../../../pyproject.toml) and successfully installed.
*   - [✅] The `TradingEnv` ([`reinforcestrategycreator/trading_environment.py`](../../../reinforcestrategycreator/trading_environment.py)) is successfully registered and usable with RLlib.
*   - [✅] The `StrategyAgent` ([`reinforcestrategycreator/rl_agent.py`](../../../reinforcestrategycreator/rl_agent.py)) logic is integrated with RLlib, either by adapting the existing agent or by configuring an RLlib built-in agent (e.g., DQN/Ape-X) with the project's neural network architecture.
*   - [✅] The main training script ([`train.py`](../../../train.py)) is refactored to use RLlib's API for parallel training (e.g., `AlgorithmConfig`, `Algorithm.train()`).
*   - [✅] The system can run multiple training environments in parallel, utilizing multiple CPU cores. (RLlib configured in train.py)
*   - [✅] Experiences from parallel environments are correctly collected and used for training the agent. (Handled by RLlib)
*   - [🟡] Database logging (for runs, episodes, steps, trades) functions correctly in the parallel setup, avoiding race conditions or data corruption. Data should be consistent with the single-process version. (Initial run logging in place, detailed episode/step logging via callbacks is TODO)
*   - [✅] Model checkpointing and saving are handled correctly by RLlib or a custom solution integrated with it. (RLlib's algo.save() used)
*   - [ ] The parallel training process completes successfully and produces results comparable to or better than the sequential version (in terms of agent performance, though faster).
*   - [ ] Basic performance metrics (e.g., total training time, steps per second) show improvement over the sequential version.
*   - [ ] Code is well-documented, especially the Ray/RLlib integration parts.

## Implementation Notes / Sub-Tasks 📝

*   - [ ] **Setup & Dependencies:**
    *   - [✅] Add `ray[rllib]` to `pyproject.toml`.
    *   - [✅] Run `poetry lock` and `poetry install`.
*   - [ ] **Environment Integration:**
    *   - [✅] Review `TradingEnv` for any necessary adjustments for RLlib compatibility.
    *   - [✅] Implement environment registration with RLlib.
*   - [✅] **Agent Integration/Configuration:**
    *   - [✅] Evaluate using RLlib's DQN/Ape-X agent vs. adapting custom `StrategyAgent`. (Decision: Use RLlib's DQN)
    *   - [✅] Configure the chosen agent, including network architecture and hyperparameters. (Done in train.py)
*   - [✅] **Training Script (`train.py`) Refactor:**
    *   - [✅] Initialize Ray (`ray.init()`).
    *   - [✅] Set up `AlgorithmConfig` (or Tuner/Trainer in newer Ray versions).
        *   [✅] Specify number of rollout workers.
        *   [✅] Configure resources per worker.
        *   [✅] Set framework to 'tf2' if using TensorFlow.
    *   - [✅] Implement the training loop using `algorithm.train()`.
*   - [🟡] **Logging Strategy:**
    *   - [🟡] Design and implement logging for parallel workers (e.g., using RLlib callbacks, custom metrics, or a central logging actor). (Initial design in train.py, callbacks are TODO)
    *   - [🟡] Ensure `run_id`, `episode_id` are handled correctly. (Partially handled for TrainingRun, further work in callbacks)
*   - [✅] **Model Management:**
    *   - [✅] Configure RLlib checkpointing. (Done via algo.save())
    *   - [✅] Ensure final model saving works as expected. (Done via algo.save())
*   - [ ] **Testing & Validation:**
    *   - [ ] Run short training sessions to verify functionality.
    *   - [ ] Compare results (e.g., final portfolio value, rewards) with the sequential version to ensure correctness.
    *   - [ ] Profile training performance.

## Diagrams 📊 (Optional)

(Refer to diagram in [TASK-ARCH-20250513-090800](./../ARCH_ParallelTraining/TASK-ARCH-20250513-090800.md))

## AI Prompt Log 🤖 (Optional)

*   (Log key prompts and AI responses during implementation)

## Review Notes 👀 (For Reviewer)

*   (Space for feedback)

## Key Learnings 💡 (Optional - Fill upon completion)

*   (Summarize discoveries)