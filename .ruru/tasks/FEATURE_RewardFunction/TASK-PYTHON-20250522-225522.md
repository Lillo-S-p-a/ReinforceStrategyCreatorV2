+++
id = "TASK-PYTHON-20250522-225522"
title = "Enhanced Reward Function for Trading Environment"
status = "ðŸŸ¢ Done"
type = "ðŸŒŸ Feature"
priority = "ðŸ”´ High"
created_date = "2025-05-22"
updated_date = "2025-05-22 23:03"
assigned_to = "dev-python"
parent_task = ""
depends_on = []
related_docs = ["reinforcestrategycreator/trading_environment.py", "tests/test_trading_env_reward.py"]
tags = ["reinforcement-learning", "reward-function", "sharpe-ratio", "unit-testing"]
template_schema_doc = ".ruru/templates/toml-md/01_mdtm_feature.README.md"
+++

# Enhanced Reward Function for Trading Environment

## Description âœï¸

* **What is this feature?** Implementing an enhanced reward function for the trading environment that combines Sharpe ratio (70%) and PnL (30%) components with a drawdown penalty
* **Why is it needed?** To improve trading model performance by creating a more balanced reward signal that considers both risk-adjusted returns and absolute profit
* **Scope:** Modify the trading environment's state tracking and reward calculation, and create comprehensive unit tests to validate the behavior
* **Branch:** mvp-rl-upgrade

## Acceptance Criteria âœ…

* - [âœ…] Extended internal state tracking in TradingEnv.reset() method:
  * - [âœ…] Added a list/queue for storing recent returns for rolling Sharpe calculation
  * - [âœ…] Added portfolio peak value tracking for drawdown calculation
* - [âœ…] Implemented enhanced reward function (_calculate_reward method):
  * - [âœ…] Sharpe component: average of last 60 returns divided by their std. dev (70% of reward)
  * - [âœ…] PnL component: change in equity divided by initial capital (30% of reward)
  * - [âœ…] Drawdown penalty: applies when drawdown exceeds 5% of historical peak
* - [âœ…] Added configurable parameters for future HPO:
  * - [âœ…] Sharpe weight (range: 0.5 to 0.9)
  * - [âœ…] PnL weight (complement to 1 of Sharpe weight)
  * - [âœ…] Sharpe window length (options: 30, 45, or 60 steps)
  * - [âœ…] Drawdown threshold (range: 3% to 7%)
  * - [âœ…] Penalty coefficient (range: 0.1% to 0.5% per point above threshold)
* - [âœ…] Created unit tests for reward validation:
  * - [âœ…] Test 1: Winning trade scenario with positive cumulative reward
  * - [âœ…] Test 2: Losing trade with drawdown exceeding threshold producing negative reward
  * - [âœ…] Test 3: No-action scenario with flat prices yielding near-zero reward
* - [âœ…] Verified reward behavior through logs in a debug episode
* - [âœ…] Confirmed all tests pass with pytest

## Implementation Notes / Sub-Tasks ðŸ“

* - [âœ…] First, analyze existing trading_environment.py structure, especially reset() and _calculate_reward() methods
* - [âœ…] Extend TradingEnv.__init__ to accept new config parameters for reward calculation
* - [âœ…] Modify reset() method to initialize new state variables
* - [âœ…] Reimplement _calculate_reward() method with the new composite formula
* - [âœ…] Create new unit tests in tests/test_trading_env_reward.py
* - [âœ…] Run manual verification with a debug episode
* - [âœ…] Execute pytest on the new test suite
* - [âœ…] Git add/commit with appropriate message

## Technical Details

### Reward Formula Structure

```
reward = (sharpe_weight * sharpe_component) + (pnl_weight * pnl_component) - drawdown_penalty

Where:
- sharpe_component = mean(recent_returns) / std(recent_returns) if std > 0, else small negative value
- pnl_component = (portfolio_value - last_portfolio_value) / initial_balance
- drawdown_penalty = max(0, current_drawdown - drawdown_threshold) * penalty_coefficient
```

### Configurable Parameters

These parameters should be exposed through the environment's config dictionary:

```python
# Default values
self.sharpe_weight = env_config.get("sharpe_weight", 0.7)  # Range: 0.5 to 0.9
self.pnl_weight = 1 - self.sharpe_weight  # Automatically calculated
self.sharpe_window_size = env_config.get("sharpe_window_size", 60)  # Options: 30, 45, 60
self.drawdown_threshold = env_config.get("drawdown_threshold", 0.05)  # Range: 0.03 to 0.07
self.drawdown_penalty_coefficient = env_config.get("drawdown_penalty_coefficient", 0.002)  # Range: 0.001 to 0.005
```

## Log Entries ðŸªµ

* [2025-05-22] Task created and assigned to dev-python for implementation
* [2025-05-22] Implemented enhanced reward function with Sharpe ratio (70%) and PnL (30%) components, plus drawdown penalty
* [2025-05-22] Added configurable parameters for future hyperparameter optimization
* [2025-05-22] Created comprehensive unit tests for the new reward function
* [2025-05-22] All tests for the enhanced reward function are passing
* [2025-05-22] Task completed and marked as done