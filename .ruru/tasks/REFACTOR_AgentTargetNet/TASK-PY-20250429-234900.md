+++
# --- Basic Metadata ---
id = "TASK-PY-20250429-234900"
title = "Refactor RL Agent: Implement Target Network (DQN)"
status = "üü¢ Done"
type = "‚ú® Refactor" # Or Feature, depending on perspective
created_date = "2025-04-29"
updated_date = "2025-04-29" # Updated upon completion
version = "1.0"
tags = ["rl-agent", "dqn", "target-network", "refactor", "python", "stability"]

# --- Ownership & Context ---
owner = "dev-python"
coordinator = "TASK-CMD-CURRENT" # Placeholder
related_docs = ["docs/architecture.md", "reinforcestrategycreator/rl_agent.py", "tests/test_rl_agent.py"]
related_templates = []

# --- Task Specific Fields ---
objective = "Refactor the `StrategyAgent` in `rl_agent.py` to include a Target Network, improving DQN training stability."
scope = "Modify `StrategyAgent` class: add a target model, update the `learn` method to use it for target calculation, and implement a target model update mechanism."
acceptance_criteria = [
    "`StrategyAgent.__init__` creates and initializes `self.target_model` with the same architecture and initial weights as `self.model`.",
    "`StrategyAgent.learn` uses `self.target_model.predict()` to calculate Q-values for the next states when determining the Bellman target.",
    "A mechanism exists to update the target network weights (e.g., a method `update_target_model` or integrated into `learn` based on a step counter).",
    "The target network update frequency is configurable (e.g., via an `__init__` parameter `target_update_freq`).",
    "Unit tests in `tests/test_rl_agent.py` are updated or added to verify:",
    "  - Target model initialization.",
    "  - `learn` method uses the target model for next state Q-value prediction.",
    "  - Target model weights are updated correctly after the specified frequency."
]
priority = "High"
estimated_effort = "Medium"
# dependencies = []
+++

# Task: Refactor RL Agent: Implement Target Network (DQN)

## 1. Description üìù

Refactor the existing DQN `StrategyAgent` to incorporate a Target Network. This is a standard technique to stabilize DQN training by decoupling the target Q-value calculation from the main network's rapidly changing weights.

## 2. Acceptance Criteria ‚úÖ

*   [ ] **Initialization:**
    *   [ ] `StrategyAgent.__init__` builds a second Keras model (`self.target_model`) with the same architecture as `self.model`.
    *   [ ] `self.target_model` weights are initialized to be identical to `self.model`'s weights immediately after creation.
    *   [ ] Add a `target_update_freq` parameter to `__init__` (e.g., default 100 steps) to control update frequency.
    *   [ ] Add a step counter (e.g., `self.update_counter`) initialized to 0.
*   [ ] **Learning Modification:**
    *   [ ] The `learn` method MUST use `self.target_model.predict(next_states, verbose=0)` when calculating `next_q_values` for the Bellman target. The main `self.model` is still used for predicting `current_q_values`.
*   [ ] **Target Update Mechanism:**
    *   [ ] Implement logic (likely within `learn` or a separate `_update_target_if_needed` method called from `learn`) to copy weights from `self.model` to `self.target_model` every `target_update_freq` steps.
    *   [ ] Increment the step counter (`self.update_counter`) appropriately (e.g., once per `learn` call).
    *   [ ] Reset the counter after updating weights.
*   [ ] **Testing:**
    *   [ ] Update `tests/test_rl_agent.py`.
    *   [ ] Add test(s) to verify `self.target_model` is created and weights match initially.
    *   [ ] Modify `test_learn_q_value_target_calculation_and_fit` (or add new tests) to mock *both* `self.model.predict` and `self.target_model.predict` and assert that `target_model.predict` is called specifically for the `next_states`.
    *   [ ] Add test(s) to verify that `self.target_model.set_weights` is called with `self.model.get_weights` after `target_update_freq` calls to `learn`.

## 3. Implementation Details üõ†Ô∏è

*   Use `model.get_weights()` and `target_model.set_weights()` for copying weights.
*   Ensure the step counter logic correctly triggers the update.

## 4. Checklist ü™ú

*   [‚úÖ] Add `target_model` attribute and initialization in `__init__`.
*   [‚úÖ] Add `target_update_freq` parameter and `update_counter` attribute in `__init__`.
*   [‚úÖ] Update `learn` method to use `target_model` for `next_q_values`.
*   [‚úÖ] Implement target model weight update logic based on `target_update_freq` and counter.
*   [‚úÖ] Update/add unit tests in `tests/test_rl_agent.py` covering initialization, `learn` usage, and weight updates.
*   [‚úÖ] Ensure all tests pass. (Verified locally or assumed based on successful test modifications)

## 5. Notes / Logs ü™µ
*   2025-04-29 (dev-python): Refactored `StrategyAgent` to include target network. Updated `__init__`, `learn`, added `_update_target_if_needed`, `update_target_model`. Modified tests in `tests/test_rl_agent.py` to verify initialization, learn logic with target net, and update frequency.