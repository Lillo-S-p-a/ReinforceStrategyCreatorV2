+++
id = "TASK-BE-20250515-155748"
title = "Fix Replay Buffer Rewards Issue in RLlib Integration"
status = "üü¢ Done"
type = "üêû Bug"
assigned_to = "dev-python"
coordinator = "roo-commander"
created_date = "2025-05-15T15:57:48Z"
updated_date = "2025-05-15T16:21:15Z" # Updated timestamp
related_docs = [".ruru/tasks/REFACTOR_RLlib_Ray_Pytorch/TASK-ARCH-20250515-144808.md"]
tags = ["rllib", "ray", "pytorch", "parallel-training", "replay-buffer", "bug", "fixed"]
+++

# Fix Replay Buffer Rewards Issue in RLlib Integration

## üéØ Goal
Fix the replay buffer rewards issue in the RLlib integration that's causing an `IndexError: index -1 is out of bounds for axis 0 with size 0` error during training.

## üìù Description
During testing of the RLlib/Ray/PyTorch parallel training refactoring, we encountered an error in the replay buffer sampling code. The error occurs because the rewards list is empty when the replay buffer tries to sample from it. This needs to be fixed to enable successful training runs.

Debug logs show:
```
[DEBUG-RL] SAMPLE: Original Episode ID: 5c4c7fc0841e4211abadca28c1a5e1ed, Sampled Timestep (ts): 29
[DEBUG-RL] SAMPLE: Calculated actual_n_step: 0
[DEBUG-RL] SAMPLE: Sampled Slice ID: 5c4c7fc0841e4211abadca28c1a5e1ed, Sampled Slice Length: 0
[DEBUG-RL] SAMPLE: Raw rewards from sampled_episode.get_rewards(): []
```

The error occurs in the `_sample_episodes` method of the `EpisodeReplayBuffer` class:
```python
rewards = scipy.signal.lfilter(
    # Error: IndexError: index -1 is out of bounds for axis 0 with size 0
)
```

## ‚úÖ Acceptance Criteria
- Identify the root cause of the empty rewards list in the replay buffer.
- Implement a fix that ensures rewards are properly stored and accessible in the replay buffer.
- Verify that the training script can run without the `IndexError` related to empty rewards.
- Document the issue and solution for future reference.

## üìö Related Documents & Context
- `reinforcestrategycreator/trading_environment.py`: Contains the trading environment implementation.
- `train.py`: Contains the main training script using RLlib and Ray.
- `.ruru/tasks/REFACTOR_RLlib_Ray_Pytorch/TASK-ARCH-20250515-144808.md`: Parent task for the overall refactoring effort.

## üìã Checklist
- [‚úÖ] Analyze the replay buffer code in RLlib to understand how rewards are stored and accessed.
- [‚úÖ] Examine the `TradingEnv` implementation to see how rewards are calculated and returned.
- [‚úÖ] Identify why the rewards list is empty during sampling.
- [‚úÖ] Implement a fix to ensure rewards are properly stored in the replay buffer.
- [‚úÖ] Test the fix by running the training script.
- [‚úÖ] Document the issue and solution.

## ü™µ Log / Notes
- 2025-05-15 15:57:48 (roo-commander): Task created and assigned to `lead-backend`.
- 2025-05-15 16:05:53 (roo-commander): Task reassigned from `lead-backend` to `dev-python` as per user request.
- 2025-05-15 16:09:00 (dev-python): Identified that the `IndexError` during replay buffer sampling was due to `n_step: 0` in the `EpisodeReplayBuffer` configuration in `train.py`. This caused `actual_n_step: 0` and an empty rewards list when `scipy.signal.lfilter` was called.
- 2025-05-15 16:09:00 (dev-python): Fixed the issue by changing `n_step` to `1` in `replay_buffer_config` within `train.py`.
- 2025-05-15 16:10:00 (dev-python): Second attempt to fix by also changing `n_step` to `1` in the main `DQNConfig().training()` block in `train.py` for consistency. This still resulted in the `IndexError`.
- 2025-05-15 16:18:00 (dev-python): Identified a `NameError: name 'force_terminate_due_to_shutdown' is not defined` in `trading_environment.py` during the previous test run. Corrected to use `self.graceful_shutdown_signaled`.
- 2025-05-15 16:21:15 (dev-python): Training script completed successfully after applying both `n_step` fixes in `train.py` and the `NameError` fix in `trading_environment.py`. The replay buffer `IndexError` is resolved.