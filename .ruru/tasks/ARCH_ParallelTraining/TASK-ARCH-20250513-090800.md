+++
id = "TASK-ARCH-20250513-090800"
title = "Analyze and Design Parallelization Strategy for Training Environments"
status = "ðŸŸ¢ Done"
type = "ðŸ”¬ Analysis & Design"
assigned_to = "core-architect"
coordinator = "roo-commander"
created_date = "2025-05-13T09:08:00Z"
updated_date = "2025-05-13T11:11:30Z"
related_docs = [
    "train.py",
    "reinforcestrategycreator/rl_agent.py",
    "reinforcestrategycreator/trading_environment.py",
    "README.md",
    ".ruru/decisions/ADR-001_Parallel_Training_Strategy.md"
]
tags = ["performance", "parallelization", "training", "architecture", "ray", "dask", "multiprocessing"]
+++

# Analyze and Design Parallelization Strategy for Training Environments

## 1. Description

The current training process for reinforcement learning agents is too long, leading to slow development iterations. This task is to analyze the existing training pipeline and design a robust strategy to parallelize the training environments. The goal is to significantly reduce overall training time.

## 2. Objectives

*   Analyze the current training implementation in [`train.py`](train.py), [`reinforcestrategycreator/rl_agent.py`](reinforcestrategycreator/rl_agent.py), and [`reinforcestrategycreator/trading_environment.py`](reinforcestrategycreator/trading_environment.py) to identify bottlenecks and areas suitable for parallelization.
*   Design a parallelization strategy for the training environments.
*   Evaluate and recommend specific tools/libraries for implementing the parallelization, explicitly considering:
    *   Ray
    *   Dask
    *   Python's `multiprocessing` module
    *   Other relevant technologies if applicable.
*   Outline the proposed architectural changes.
*   Provide a high-level plan for implementation, including potential challenges and considerations.

## 3. Acceptance Criteria

*   A written report detailing the analysis of the current system.
*   A clear description of the proposed parallelization architecture.
*   Justification for the recommended tools/libraries.
*   High-level sequence diagram or component diagram of the proposed solution.
*   Identification of key changes required in the existing codebase.
*   An ADR (Architecture Decision Record) draft for the chosen parallelization approach.

## 4. Checklist

*   [âœ…] Review existing codebase: [`train.py`](train.py), [`reinforcestrategycreator/rl_agent.py`](reinforcestrategycreator/rl_agent.py), [`reinforcestrategycreator/trading_environment.py`](reinforcestrategycreator/trading_environment.py).
*   [âœ…] Identify current performance bottlenecks in the training loop. (Initial thought: sequential episode execution is the primary bottleneck).
*   [âœ…] Research and compare parallelization options (Ray, Dask, multiprocessing, others).
*   [âœ…] Design the parallel training architecture.
*   [âœ…] Document the chosen approach, including diagrams. (Initial diagram in task notes, ADR contains formal decision)
*   [âœ…] Draft an ADR for the parallelization strategy. (See [ADR-001_Parallel_Training_Strategy.md](../decisions/ADR-001_Parallel_Training_Strategy.md))
*   [âœ…] Outline implementation steps and potential risks.
*   [ ] Prepare a summary report for review.

## 5. Notes & Logs

*(Space for `core-architect` to add notes during execution)*

**2025-05-13 Core Architect Notes:**
*   Initial code review completed.
*   `train.py` runs episodes sequentially. Each episode consists of multiple steps within `TradingEnv`.
*   `StrategyAgent` (DQN) learns from experiences.
*   Primary bottleneck appears to be the sequential execution of training episodes.
*   Potential parallelization points:
    *   Running multiple `TradingEnv` instances (episodes) concurrently.
    *   Managing shared `StrategyAgent` learning from parallel experience generation.
    *   Addressing potential DB contention from concurrent logging.

**2025-05-13 Core Architect Research Notes (Parallelization Options):**
*   **Ray (RLlib):**
    *   High-level framework specifically designed for RL.
    *   Provides `RLlib` for managing parallel environments and distributed data collection.
    *   Supports a wide range of RL algorithms.
    *   Good scalability from local cores to clusters.
    *   Likely offers the most out-of-the-box support for this task.
*   **Dask:**
    *   General-purpose parallel computing library.
    *   Can parallelize environment interactions using `dask.delayed`.
    *   `Dask DataFrames` can manage large collected datasets.
    *   Requires more manual implementation of RL-specific logic (experience aggregation, agent learning synchronization) compared to RLlib.
*   **Python `multiprocessing`:**
    *   Standard library for process-based parallelism.
    *   Can run multiple environment instances in separate processes using `multiprocessing.Pool`.
    *   Requires explicit data sharing mechanisms (Queues, Pipes) for experience collection.
    *   Most manual effort to implement the full parallel RL training loop, including agent synchronization and model updates.

**2025-05-13 Core Architect Design Notes (Ray-based Parallelization):**

**Proposed Ray-based Architecture:**

1.  **Central Learner Agent (`StrategyAgent` - DQN):**
    *   Resides in the main process or a dedicated Ray actor.
    *   Manages the replay buffer, neural network, and optimizer.
2.  **Parallel Rollout Workers (Ray Actors):**
    *   Multiple Ray actors, each running an instance of `TradingEnv`.
    *   Each worker uses a copy of the current policy from the central learner to generate experiences.
3.  **Experience Collection:**
    *   Workers send collected experiences (`state`, `action`, `reward`, `next_state`, `done`) to the central learner's replay buffer.
    *   Ray's actor communication mechanisms will be used.
4.  **Agent Learning & Policy Updates:**
    *   The central learner samples from the replay buffer to train (`agent.learn()`).
    *   Updated model weights/policy are periodically broadcast or made available to rollout workers (RLlib typically handles this).
5.  **Database Logging:**
    *   Consider a centralized logging actor or careful management of DB connections if workers log directly to avoid contention. Alternatively, workers send summary data to the main process/learner for logging.
6.  **Model Saving:**
    *   The central learner is responsible for saving model weights.

**Conceptual Diagram (Mermaid):**
```mermaid
graph TD
    subgraph CentralProcess["Central Process / Ray Driver"]
        Learner[StrategyAgent (DQN Learner)\n- Replay Buffer\n- Neural Network\n- Optimizer]
        ModelStore["Model Storage (e.g., .keras files)"]
        DBLogger["Database Logger (Centralized or Coordinated)"]
    end

    subgraph RayRolloutWorkers["Ray Rollout Workers (Actors)"]
        RW1["Worker 1\n- TradingEnv Instance\n- Policy Copy"]
        RW2["Worker 2\n- TradingEnv Instance\n- Policy Copy"]
        RWN["Worker N\n- TradingEnv Instance\n- Policy Copy"]
    end

    Learner -- "Samples Batch" --> Learner
    Learner -- "Updates Weights" --> Learner
    Learner -- "Saves Model" --> ModelStore
    Learner -- "Sends Updated Policy" --> RW1
    Learner -- "Sends Updated Policy" --> RW2
    Learner -- "Sends Updated Policy" --> RWN

    RW1 -- "Generates Experiences (s,a,r,s',d)" --> Learner
    RW2 -- "Generates Experiences (s,a,r,s',d)" --> Learner
    RWN -- "Generates Experiences (s,a,r,s',d)" --> Learner

    RW1 -- "Logs Data (Optional: Direct or via Central)" --> DBLogger
    RW2 -- "Logs Data (Optional: Direct or via Central)" --> DBLogger
    RWN -- "Logs Data (Optional: Direct or via Central)" --> DBLogger

    %% Interactions with existing components
    RW1 --> TradingEnv1[TradingEnv]
    RW2 --> TradingEnv2[TradingEnv]
    RWN --> TradingEnvN[TradingEnv]

    TradingEnv1 --> Data[Historical Data + Indicators]
    TradingEnv2 --> Data
    TradingEnvN --> Data
```
This design leverages Ray's actor model for parallelism. RLlib, a library within Ray, would likely be used to implement this, simplifying many aspects of distributed RL.

**2025-05-13 Core Architect Implementation Outline & Risks:**

**High-Level Implementation Steps (Ray/RLlib):**

1.  **Dependency Management:** Add `ray[rllib]` to [`pyproject.toml`](../../pyproject.toml) and install.
2.  **Environment Adaptation:** Ensure `TradingEnv` ([`reinforcestrategycreator/trading_environment.py`](../../reinforcestrategycreator/trading_environment.py)) is compatible and register with RLlib.
3.  **Agent Configuration:**
    *   Option A: Adapt existing `StrategyAgent` ([`reinforcestrategycreator/rl_agent.py`](../../reinforcestrategycreator/rl_agent.py)) for RLlib custom models.
    *   Option B (Recommended): Use RLlib's built-in DQN/Ape-X agent, configured with our network architecture.
4.  **Training Script Refactor ([`train.py`](../../train.py)):**
    *   Initialize Ray (`ray.init()`).
    *   Use RLlib's `AlgorithmConfig` to define environment, agent, workers, resources.
    *   Replace manual loop with `Algorithm.train()`.
5.  **Logging Integration:**
    *   Use RLlib callbacks (`DefaultCallbacks` or custom) for DB logging.
    *   Coordinate `run_id`, `episode_id` generation. Consider a central logging actor or careful per-worker session management.
6.  **Model Checkpointing:** Utilize RLlib's mechanisms for model saving/loading.
7.  **Testing & Profiling:** Thoroughly test parallel setup and profile performance.

**Potential Risks & Considerations:**

*   **Learning Curve:** New framework (Ray/RLlib).
*   **Debugging Complexity:** Distributed systems debugging. Ray Dashboard will be essential.
*   **Resource Consumption:** Increased CPU/memory usage.
*   **Database Contention:** Requires robust logging strategy for parallel workers.
*   **Hyperparameter Sensitivity:** Parallelism might necessitate re-tuning.
*   **Determinism:** May be harder to achieve perfect run-to-run determinism.

**2025-05-13 Core Architect Summary Report:**

**Objective:** Analyze the current training pipeline and design a robust strategy to parallelize training environments to reduce overall training time.

**Analysis:**
*   Reviewed codebase: [`train.py`](../../train.py), [`reinforcestrategycreator/rl_agent.py`](../../reinforcestrategycreator/rl_agent.py), [`reinforcestrategycreator/trading_environment.py`](../../reinforcestrategycreator/trading_environment.py).
*   Identified the primary bottleneck as the sequential execution of training episodes in `train.py`.

**Parallelization Options Researched:**
1.  **Ray (RLlib):** High-level RL-specific framework, good for parallel environments and distributed data collection.
2.  **Dask:** General-purpose parallel computing, requires more manual RL implementation.
3.  **Python `multiprocessing`:** Standard library, most manual effort.

**Decision & Proposed Architecture:**
*   **Chosen Tool:** Ray, specifically its RLlib library.
*   **Rationale:** RLlib offers the best balance of RL-specific features, scalability, and reduced custom implementation effort.
*   **Architecture:**
    *   Central Learner `StrategyAgent` (DQN).
    *   Multiple Ray actors as Rollout Workers, each running a `TradingEnv` instance.
    *   Experiences collected by workers are sent to the learner's replay buffer.
    *   Learner updates policy, which is distributed to workers.
    *   Database logging and model saving coordinated by the central learner/driver or dedicated actors.
*   **ADR:** Documented in [ADR-001_Parallel_Training_Strategy.md](../decisions/ADR-001_Parallel_Training_Strategy.md).

**High-Level Implementation Plan:**
1.  Integrate Ray/RLlib dependency.
2.  Adapt `TradingEnv` for RLlib.
3.  Configure `StrategyAgent` within RLlib (likely using RLlib's DQN with our network).
4.  Refactor `train.py` to use RLlib's training paradigm.
5.  Implement robust parallel logging and model saving.

**Key Risks:** Learning curve for Ray/RLlib, debugging complexity, resource management, DB contention.

**Expected Outcome:** Significant reduction in training time, enabling faster iteration cycles.