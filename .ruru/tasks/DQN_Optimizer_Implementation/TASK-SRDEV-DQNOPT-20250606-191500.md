+++
id = "TASK-SRDEV-DQNOPT-20250606-191500"
title = "Implement Adam Optimizer & Backpropagation in DQN"
status = "ðŸŸ¢ Done"
type = "ðŸŒŸ Feature"
assigned_to = "util-senior-dev"
coordinator = "TASK-CMD-DQNOPT-20250606-191500"
created_date = "2025-06-06T19:15:00Z"
updated_date = "2025-06-06T19:14:00Z"
related_docs = [
    "reinforcestrategycreator_pipeline/src/models/implementations/dqn.py"
]
tags = ["dqn", "optimizer", "backpropagation", "adam", "machine-learning", "refactor", "numpy"]
+++

## Description

The current DQN model in [`dqn.py`](reinforcestrategycreator_pipeline/src/models/implementations/dqn.py) uses a placeholder mechanism for updating network weights in the `_train_step()` method. This involves adding scaled random noise and does not allow for effective learning.

This task is to replace this placeholder with a proper gradient-based optimization algorithm, specifically Adam, and implement the necessary backpropagation to calculate gradients. The implementation should be done in pure NumPy, consistent with the existing model structure.

The current network is a 2-layer MLP with ReLU activation on the hidden layer and a linear output layer, as seen in the `_forward()` method:
- Input -> Dense(hidden_units) -> ReLU -> Dense(action_size) -> Q-values
- Weights: `W0`, `b0` (input to hidden), `W1`, `b1` (hidden to output)

The loss is Mean Squared Error (MSE) between target Q-values and predicted Q-values for selected actions, already calculated in `_train_step()`.

## Acceptance Criteria

1.  The placeholder weight update logic in `_train_step()` is completely removed.
2.  Backpropagation is implemented in pure NumPy to calculate gradients of the MSE loss with respect to all network weights (`W0`, `b0`, `W1`, `b1`).
    *   The derivatives for ReLU and linear layers must be correctly applied.
3.  The Adam optimization algorithm is implemented in pure NumPy and used to update network weights.
    *   It should use `self.learning_rate`.
    *   Standard Adam hyperparameters (`beta1`, `beta2`, `epsilon`) should be added as attributes to the `DQN` class (e.g., initialized in `__init__` with common default values like `beta1=0.9`, `beta2=0.999`, `epsilon=1e-8`).
    *   Moving averages for gradients (m) and squared gradients (v) required by Adam must be initialized (e.g., as zero arrays with the same shape as weights) and maintained across training steps (e.g., stored in `self.q_network['optimizer_state']` or similar).
4.  The model structure and its integration within the existing pipeline remain unchanged.
5.  The `_train_step()` method should continue to return the calculated loss (a scalar value).
6.  After implementation, running the main pipeline should show the model training, and the loss should ideally exhibit a decreasing trend over episodes/epochs, indicating learning. Perfect convergence is not an immediate AC, but the mechanism for learning must be functional.
7.  The implementation must be robust against potential numerical issues if any are anticipated during gradient calculation or optimizer updates, though the primary focus is on correct implementation.

## Checklist

-   [x] Analyze the existing `_train_step()` and `_forward()` methods in [`dqn.py`](reinforcestrategycreator_pipeline/src/models/implementations/dqn.py) to fully understand data flow and current weight structure.
-   [x] Add Adam hyperparameters (`beta1`, `beta2`, `epsilon`) and state variables (for 1st and 2nd moment estimates `m` and `v` for each weight matrix) to the `DQN` class `__init__` method and initialize them appropriately (e.g., in `self.q_network['optimizer_state']`).
-   [x] Implement the backward pass (backpropagation) for the 2-layer MLP within `_train_step()`:
    -   [x] Calculate gradient of the loss with respect to the output of the network (`q_values` for selected actions).
    -   [x] Backpropagate gradients through the final linear layer to get gradients for `W1` and `b1`.
    -   [x] Backpropagate gradients through the ReLU activation.
    -   [x] Backpropagate gradients through the first linear layer to get gradients for `W0` and `b0`.
-   [x] Implement the Adam optimizer update rule for each weight matrix (`W0`, `b0`, `W1`, `b1`) using the calculated gradients, learning rate, and Adam state variables.
-   [x] Replace the old placeholder random weight update logic entirely with the new backpropagation and Adam update steps.
-   [x] Ensure `self.learning_rate` is correctly used by the Adam optimizer.
-   [x] Verify that updates are applied to `self.q_network['weights']`.
-   [x] Add concise logging for key parts of the new update mechanism (e.g., calculated loss, norms of gradients if feasible without excessive computation, magnitude of weight updates for one or two key weights as a sanity check).
-   [ ] Thoroughly test the changes by running the main pipeline script (`reinforcestrategycreator_pipeline/run_main_pipeline.py`) and observing the loss trend.
-   [ ] Ensure no new NaN/Inf issues are introduced.

## Notes
- The implementation must be in pure NumPy.
- Activation function for hidden layer is ReLU: `h = np.maximum(0, z0)`. Derivative is 1 if `z0 > 0`, else 0.
- Output layer is linear.
- Loss function is MSE: `loss = np.mean((targets - current_q_selected)**2)`. Gradient of loss w.r.t `current_q_selected` is `2 * (current_q_selected - targets) / batch_size`.