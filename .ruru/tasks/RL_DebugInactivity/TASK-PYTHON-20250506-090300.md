+++
# --- MDTM Task File ---
id = "TASK-PYTHON-20250506-090300"
title = "Investigate Agent Inactivity (Not Taking Trading Actions)"
status = "‚ö™ Blocked" # Options: üü° To Do, üü† In Progress, üü¢ Done, ‚ö™ Blocked, üü£ Review
type = "üêû Bug" # Investigating unexpected behavior
created_date = "2025-05-06"
updated_date = "2025-05-06" # Updated due to rate limit
assigned_to = "dev-python" # Mode slug
coordinator = "TASK-CMD-..." # Replace with actual Commander Task ID if available
priority = "Critical" # Blocking further progress
complexity = "High" # Requires deep dive into agent/env interaction
estimated_effort = "3h"
related_tasks = ["TASK-PYTHON-20250506-005600"] # Follows the previous tuning attempt
target_branch = "feature/rl-strategy-enhancements"
tags = ["rl", "trading", "debugging", "agent-behavior", "inactivity", "python"]
# --- End Metadata ---
+++

# Investigate Agent Inactivity (Not Taking Trading Actions)

## 1. Description

The user observed that during recent training runs (even after Phase 1 enhancements and initial tuning), the RL agent appears to be inactive, consistently choosing the 'Flat' action and not entering Long or Short positions.

This task requires a detailed investigation into the agent's decision-making process and the environment interaction to determine the cause of this inactivity.

Focus areas:
1.  Confirm the inactivity by running a short training (10 episodes) and analyzing the `trading_operations` in the database for the resulting run.
2.  Analyze Q-Values: Add logging within `rl_agent.py`'s `select_action` method to inspect the predicted Q-values for each action (Flat, Long, Short) just before the `argmax` selection. Is the Q-value for 'Flat' consistently the highest?
3.  Analyze Rewards: Examine the rewards being generated by the environment (`trading_environment.py`). Are the rewards predominantly negative or zero, potentially teaching the agent that *any* action is worse than doing nothing? Check the balance of reward components (Sharpe, penalties).
4.  Check Exploration (Epsilon): Verify the epsilon value and decay (`AGENT_EPSILON`, `AGENT_EPSILON_DECAY`, `AGENT_EPSILON_MIN`). Is exploration sufficient, or is epsilon decaying too quickly, causing the agent to get stuck in exploitation mode early?
5.  Review State/Normalization: Double-check the state representation being fed to the agent and the normalization process. Are there any anomalies or issues that might lead to poor Q-value predictions?
6.  Review Risk Management: Are the stop-loss or other risk parameters overly restrictive, making potential entries immediately unattractive based on predicted risk?
7.  Review Agent Learning: Is the `agent.learn()` method being called frequently enough? Are the target network updates happening correctly?

## 2. Acceptance Criteria

*   A short training run (10 episodes) is executed to reproduce and confirm the inactivity.
*   Analysis of Q-values, rewards, epsilon, state, and risk parameters is performed.
*   The root cause(s) of the agent's inactivity are identified.
*   Specific code changes or parameter adjustments to fix the inactivity are proposed or implemented.
*   Findings, analysis steps, and proposed solutions are documented in this task file.
*   Any necessary code changes are committed to the `feature/rl-strategy-enhancements` branch.

## 3. Checklist

*   [ ] Run `train.py` for 10 episodes.
*   [ ] Query `trading_operations` for the new run_id to confirm inactivity (mostly HOLD operations).
*   [‚úÖ] Add temporary logging to `rl_agent.select_action` to print Q-values before `argmax`.
*   [ ] Rerun a short training/evaluation step to observe Q-values. (Blocked by YF Rate Limit)
*   [‚úÖ] Analyze the reward calculation logic and typical reward values received by the agent.
*   [‚úÖ] Check epsilon parameters and decay schedule in `train.py` and `rl_agent.py`.
*   [‚úÖ] Review state normalization logic in `trading_environment.py`.
*   [‚úÖ] Review risk management parameters (`ENV_STOP_LOSS_PCT`, `ENV_RISK_FRACTION`) in `train.py`.
*   [‚úÖ] Review agent learning frequency (`AGENT_BATCH_SIZE`, `agent.learn()` calls).
*   [ ] Identify root cause(s).
*   [ ] Propose/implement fix(es).
*   [ ] Document findings and solution.
*   [ ] Commit changes if any code was modified.

## 4. Logs / Notes

*(Python Developer will add analysis steps, observations, and proposed solutions here)*

**2025-05-06 09:04 AM:** Attempted to run `train.py` for 10 episodes. Encountered `YFRateLimitError` from Yahoo Finance. Waiting approximately 30 minutes before retrying. Task status changed to Blocked.

**2025-05-06 09:53 AM (Current Time):**
*   Added Q-value logging to `rl_agent.py` in `select_action`.
*   Still blocked by `YFRateLimitError` for running the 10-episode test.
*   **Initial Code Review Findings & Hypotheses:**
    *   **Reward Calculation (`TradingEnv._calculate_reward`):**
        *   Reward = `risk_adjusted_return - trading_penalty - drawdown_penalty`.
        *   `risk_adjusted_return` (Sharpe or % change) might be small/negative. A previous scaling factor of `0.01` on Sharpe was removed, which could make this component's magnitude much larger.
        *   `trading_penalty` = `ENV_TRADING_PENALTY` (0.005) * `_trade_count`.
        *   `drawdown_penalty` = `ENV_DRAWDOWN_PENALTY` (0.01) * `current_drawdown`.
        *   *Hypothesis:* If `risk_adjusted_return` is often small or negative, the penalties could consistently make 'Flat' the action with the highest expected Q-value.
    *   **State Normalization (`TradingEnv._get_observation`):**
        *   Market features: Rolling z-score over `normalization_window_size` (20). Past window data normalized by *current* step's rolling stats.
        *   Account info: Normalized relative to initial balance.
        *   *Hypothesis:* Seems reasonable. Small `current_std` in normalization (though `1e-8` is added) could lead to large state values if not robust.
    *   **Parameters (`train.py`):**
        *   Epsilon (Exploration): `AGENT_EPSILON = 1.0`, `AGENT_EPSILON_DECAY = 0.995`, `AGENT_EPSILON_MIN = 0.01`. Seems standard for initial exploration.
        *   Risk Management & Penalties:
            *   `ENV_DRAWDOWN_PENALTY = 0.01`
            *   `ENV_TRADING_PENALTY = 0.005`
            *   `ENV_RISK_FRACTION = 0.02` (implies small trades)
            *   `ENV_STOP_LOSS_PCT = 5.0`
            *   *Hypothesis:* Small `ENV_RISK_FRACTION` leads to small positions. If profits from these are consistently outweighed by transaction costs (part of PnL) or the explicit penalties, it would discourage trading.
        *   Agent Learning:
            *   `AGENT_BATCH_SIZE = 32` (Standard)
            *   `AGENT_TARGET_UPDATE_FREQ = 5` (Very frequent; `rl_agent.py` constructor default was 100).
            *   *Hypothesis:* Very frequent target network updates might lead to unstable Q-value learning, causing the agent to prefer 'Flat'.

**2025-05-06 09:55 AM:**
*   Attempted to run `train.py` again. Still blocked by `YFRateLimitError`.
*   Decided to wait longer (15-30 minutes from 9:55 AM) for the rate limit to clear before making changes to `AGENT_TARGET_UPDATE_FREQ` or re-attempting training.

**2025-05-06 09:55 AM (Attempt 2):**
*   Attempted to run `train.py` again after ~20 mins. Still blocked by `YFRateLimitError`.

**2025-05-06 09:56 AM:**
*   Continuing to wait for Yahoo Finance rate limit to clear before making code changes or re-attempting training.