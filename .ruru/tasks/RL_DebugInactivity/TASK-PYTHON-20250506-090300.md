+++
# --- MDTM Task File ---
id = "TASK-PYTHON-20250506-090300"
title = "Investigate Agent Inactivity (Not Taking Trading Actions)"
status = "üü¢ Done" # Options: üü° To Do, üü† In Progress, üü¢ Done, ‚ö™ Blocked, üü£ Review
type = "üêû Bug" # Investigating unexpected behavior
created_date = "2025-05-06"
updated_date = "2025-05-06" # Updated with final resolution
assigned_to = "dev-python" # Mode slug
coordinator = "TASK-CMD-..." # Replace with actual Commander Task ID if available
priority = "Critical" # Blocking further progress
complexity = "High" # Requires deep dive into agent/env interaction
estimated_effort = "3h"
related_tasks = ["TASK-PYTHON-20250506-005600"] # Follows the previous tuning attempt
target_branch = "feature/rl-strategy-enhancements"
tags = ["rl", "trading", "debugging", "agent-behavior", "inactivity", "python"]
# --- End Metadata ---
+++

# Investigate Agent Inactivity (Not Taking Trading Actions)

## 1. Description

The user observed that during recent training runs (even after Phase 1 enhancements and initial tuning), the RL agent appears to be inactive, consistently choosing the 'Flat' action and not entering Long or Short positions.

This task requires a detailed investigation into the agent's decision-making process and the environment interaction to determine the cause of this inactivity.

Focus areas:
1.  Confirm the inactivity by running a short training (10 episodes) and analyzing the `trading_operations` in the database for the resulting run.
2.  Analyze Q-Values: Add logging within `rl_agent.py`'s `select_action` method to inspect the predicted Q-values for each action (Flat, Long, Short) just before the `argmax` selection. Is the Q-value for 'Flat' consistently the highest?
3.  Analyze Rewards: Examine the rewards being generated by the environment (`trading_environment.py`). Are the rewards predominantly negative or zero, potentially teaching the agent that *any* action is worse than doing nothing? Check the balance of reward components (Sharpe, penalties).
4.  Check Exploration (Epsilon): Verify the epsilon value and decay (`AGENT_EPSILON`, `AGENT_EPSILON_DECAY`, `AGENT_EPSILON_MIN`). Is exploration sufficient, or is epsilon decaying too quickly, causing the agent to get stuck in exploitation mode early?
5.  Review State/Normalization: Double-check the state representation being fed to the agent and the normalization process. Are there any anomalies or issues that might lead to poor Q-value predictions?
6.  Review Risk Management: Are the stop-loss or other risk parameters overly restrictive, making potential entries immediately unattractive based on predicted risk?
7.  Review Agent Learning: Is the `agent.learn()` method being called frequently enough? Are the target network updates happening correctly?

## 2. Acceptance Criteria

*   A short training run (10 episodes) is executed to reproduce and confirm the inactivity.
*   Analysis of Q-values, rewards, epsilon, state, and risk parameters is performed.
*   The root cause(s) of the agent's inactivity are identified.
*   Specific code changes or parameter adjustments to fix the inactivity are proposed or implemented.
*   Findings, analysis steps, and proposed solutions are documented in this task file.
*   Any necessary code changes are committed to the `feature/rl-strategy-enhancements` branch.

## 3. Checklist

*   [‚úÖ] Run `train.py` for 10 episodes.
*   [‚úÖ] Query `trading_operations` for the new run_id to confirm inactivity (mostly HOLD operations).
*   [‚úÖ] Add temporary logging to `rl_agent.select_action` to print Q-values before `argmax`.
*   [‚úÖ] Rerun a short training/evaluation step to observe Q-values. (Blocked by YF Rate Limit - Now unblocked and run)
*   [‚úÖ] Analyze the reward calculation logic and typical reward values received by the agent.
*   [‚úÖ] Check epsilon parameters and decay schedule in `train.py` and `rl_agent.py`.
*   [‚úÖ] Review state normalization logic in `trading_environment.py`.
*   [‚úÖ] Review risk management parameters (`ENV_STOP_LOSS_PCT`, `ENV_RISK_FRACTION`) in `train.py`.
*   [‚úÖ] Review agent learning frequency (`AGENT_BATCH_SIZE`, `agent.learn()` calls). (AGENT_TARGET_UPDATE_FREQ changed from 5 to 50 in `train.py`)
*   [‚úÖ] Identify root cause(s).
*   [‚úÖ] Propose/implement fix(es).
*   [‚úÖ] Document findings and solution.
*   [ ] Commit changes if any code was modified. (To be done next)

## 4. Logs / Notes

*(Python Developer will add analysis steps, observations, and proposed solutions here)*

**2025-05-06 09:04 AM:** Attempted to run `train.py` for 10 episodes. Encountered `YFRateLimitError` from Yahoo Finance. Waiting approximately 30 minutes before retrying. Task status changed to Blocked.

**2025-05-06 09:53 AM (Current Time):**
*   Added Q-value logging to `rl_agent.py` in `select_action`.
*   Still blocked by `YFRateLimitError` for running the 10-episode test.
*   **Initial Code Review Findings & Hypotheses:**
    *   **Reward Calculation (`TradingEnv._calculate_reward`):**
        *   Reward = `risk_adjusted_return - trading_penalty - drawdown_penalty`.
        *   `risk_adjusted_return` (Sharpe or % change) might be small/negative. A previous scaling factor of `0.01` on Sharpe was removed, which could make this component's magnitude much larger.
        *   `trading_penalty` = `ENV_TRADING_PENALTY` (0.005) * `_trade_count`.
        *   `drawdown_penalty` = `ENV_DRAWDOWN_PENALTY` (0.01) * `current_drawdown`.
        *   *Hypothesis:* If `risk_adjusted_return` is often small or negative, the penalties could consistently make 'Flat' the action with the highest expected Q-value.
    *   **State Normalization (`TradingEnv._get_observation`):**
        *   Market features: Rolling z-score over `normalization_window_size` (20). Past window data normalized by *current* step's rolling stats.
        *   Account info: Normalized relative to initial balance.
        *   *Hypothesis:* Seems reasonable. Small `current_std` in normalization (though `1e-8` is added) could lead to large state values if not robust.
    *   **Parameters (`train.py`):**
        *   Epsilon (Exploration): `AGENT_EPSILON = 1.0`, `AGENT_EPSILON_DECAY = 0.995`, `AGENT_EPSILON_MIN = 0.01`. Seems standard for initial exploration.
        *   Risk Management & Penalties:
            *   `ENV_DRAWDOWN_PENALTY = 0.01`
            *   `ENV_TRADING_PENALTY = 0.005`
            *   `ENV_RISK_FRACTION = 0.02` (implies small trades)
            *   `ENV_STOP_LOSS_PCT = 5.0`
        *   *Hypothesis:* Small `ENV_RISK_FRACTION` leads to small positions. If profits from these are consistently outweighed by transaction costs (part of PnL) or the explicit penalties, it would discourage trading.
        *   Agent Learning:
            *   `AGENT_BATCH_SIZE = 32` (Standard)
            *   `AGENT_TARGET_UPDATE_FREQ = 5` (Very frequent; `rl_agent.py` constructor default was 100).
            *   *Hypothesis:* Very frequent target network updates might lead to unstable Q-value learning, causing the agent to prefer 'Flat'.

**2025-05-06 09:55 AM:**
*   Attempted to run `train.py` again. Still blocked by `YFRateLimitError`.
*   Decided to wait longer (15-30 minutes from 9:55 AM) for the rate limit to clear before making changes to `AGENT_TARGET_UPDATE_FREQ` or re-attempting training.

**2025-05-06 09:55 AM (Attempt 2):**
*   Attempted to run `train.py` again after ~20 mins. Still blocked by `YFRateLimitError`.

**2025-05-06 09:56 AM:**
*   Continuing to wait for Yahoo Finance rate limit to clear before making code changes or re-attempting training.

**2025-05-06 15:04 PM (Europe/Rome):**
*   Attempted to run `poetry run python train.py`.
*   Encountered `ModuleNotFoundError: No module named 'pandas_ta'`. Resolved by ensuring execution within poetry environment (`poetry run`).
*   Still encountering `YFRateLimitError: Too Many Requests`.
*   Changed `AGENT_TARGET_UPDATE_FREQ` in `train.py` from `5` to `50` as per hypothesis in previous logs to potentially improve Q-value stability.
*   Task remains blocked by Yahoo Finance rate limit. Will wait before re-attempting training.

**2025-05-06 15:24 PM (Europe/Rome):**
*   `yfinance` package updated, rate limit issue seems resolved for now.
*   Successfully ran `poetry run python train.py` for 10 episodes. Run ID: `RUN-SPY-20250506130841-488b814e`.
    *   Q-value logging in `rl_agent.py` was active and printed values to the console.
    *   Initial Q-values in early steps of episode 1 showed variability, not always favoring 'Flat'.
    *   Example Q-values (Flat, Long, Short) from early in Episode 1:
        *   `[ 0.22831643 -1.4033875  -2.7363148 ]` (Selects Flat)
        *   `[-0.82860786 -1.7481692  -0.744485  ]` (Selects Short)
        *   `[-0.78770155 -0.543824   -1.2697431 ]` (Selects Long)
    *   However, Episode 10 finished with Total Reward: 0.0000 and Final Portfolio: 10000.00 (same as initial).
*   Created `check_run_operations.py` to query the database for trading operations for the new run_id.
*   Executed `check_run_operations.py RUN-SPY-20250506130841-488b814e`.
    *   Output: "No trading operations found for run_id: RUN-SPY-20250506130841-488b814e".
*   **Conclusion:** The agent inactivity (no trading operations) is confirmed even after the `yfinance` update and change to `AGENT_TARGET_UPDATE_FREQ`. The root cause likely lies in the reward structure or how the agent perceives the long-term utility of trading actions versus remaining flat, especially given the penalties and risk fraction.
*   The checklist item "Rerun a short training/evaluation step to observe Q-values" is now considered complete.
*   The checklist item "Run `train.py` for 10 episodes" is complete.
*   The checklist item "Query `trading_operations` for the new run_id to confirm inactivity" is complete.
*   Status remains "‚ö™ Blocked" as the core inactivity issue is not yet resolved, but the immediate block by YF rate limit for testing is lifted. The next step is to focus on the reward structure and potentially experiment with penalty/risk parameters.

**2025-05-06 15:45 PM (Europe/Rome):**
*   Adjusted penalties in `train.py`:
    *   `ENV_TRADING_PENALTY` from `0.005` to `0.0005`.
    *   `ENV_DRAWDOWN_PENALTY` from `0.01` to `0.001`.
*   Successfully ran `poetry run python train.py` for 10 episodes. New Run ID: `RUN-SPY-20250506132954-c5196af3`.
    *   Episode 10 finished with Total Reward: 0.0000 and Final Portfolio: 10000.00.
*   Executed `check_run_operations.py RUN-SPY-20250506132954-c5196af3`.
    *   Output: "No trading operations found for run_id: RUN-SPY-20250506132954-c5196af3".
*   **Conclusion:** Reducing `ENV_TRADING_PENALTY` and `ENV_DRAWDOWN_PENALTY` did not resolve the agent inactivity. The agent still made no trades.
*   The issue likely still resides in the overall reward formulation or other parameters like `ENV_RISK_FRACTION` being too conservative, making 'Flat' consistently appear as the optimal action.

**2025-05-06 16:02 PM (Europe/Rome):**
*   Increased `ENV_RISK_FRACTION` in `train.py` from `0.02` back to `0.1`.
*   Successfully ran `poetry run python train.py` for 10 episodes. New Run ID: `RUN-SPY-20250506134611-f4a2c7f1`.
    *   Logs showed "Stop-Loss triggered" messages, indicating trading activity.
    *   Episode 10 finished with Total Reward: -99.6438 and Final Portfolio: 9877.27 (different from initial).
*   Executed `check_run_operations.py RUN-SPY-20250506134611-f4a2c7f1`.
    *   Output showed significant trading operations: ENTRY_SHORT: 1510, EXIT_SHORT: 1507, ENTRY_LONG: 1326, EXIT_LONG: 1324.
*   **Root Cause Identified:** The primary cause of inactivity was the low `ENV_RISK_FRACTION` (0.02). This resulted in very small potential position sizes, where the expected profit was likely insufficient to overcome transaction costs (implicit in PnL) and explicit penalties, making the 'Flat' action appear optimal to the agent.
*   **Solution Implemented:** Increasing `ENV_RISK_FRACTION` to 0.1 allowed for larger position sizes, making trading actions potentially more rewarding and resolving the inactivity.
*   **Final Conclusion:** The agent inactivity bug is resolved by adjusting the `ENV_RISK_FRACTION` parameter. Further tuning of penalties and risk fraction might be needed for optimal performance, but the agent is now actively trading.
*   Checklist items for identifying root cause and implementing fix are now complete. Task status changed to Done.